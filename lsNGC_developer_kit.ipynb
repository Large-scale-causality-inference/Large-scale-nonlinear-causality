{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_0_mean_1_std(inp_series):\n",
    "    inp_series=inp_series.copy()\n",
    "    mean_ts=np.array([inp_series.mean(axis=1)]).transpose()\n",
    "    mean_ts_mtrx = mean_ts*np.ones((1,inp_series.shape[1]));\n",
    "    unb_data_mtrx = inp_series - mean_ts_mtrx\n",
    "    p = np.power(unb_data_mtrx,2)\n",
    "    s=np.array([p.sum(axis=1)]).transpose()\n",
    "    sc=np.sqrt(s/p.shape[1])\n",
    "    sc2=sc*(np.ones((1,p.shape[1])))\n",
    "    nrm= np.divide(unb_data_mtrx,sc2)\n",
    "    return nrm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 2: Get data from the file and encode the labels using LabelEncoder class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def multivariate_split(X,ar_order, valid_percent=0):\n",
    "    X=X.copy()\n",
    "    TS=np.shape(X)[1]\n",
    "    n_vars=np.shape(X)[0]\n",
    "    val_num=int(valid_percent*TS)\n",
    "    my_data_train=torch.zeros((TS-ar_order-val_num,ar_order,n_vars))\n",
    "    my_data_y_train=torch.zeros((TS-ar_order-val_num,1,n_vars))\n",
    "    my_data_val=torch.zeros((val_num,ar_order,n_vars))\n",
    "    my_data_y_val=torch.zeros((val_num,1,n_vars))\n",
    "    for i in range(TS-ar_order-val_num):\n",
    "        my_data_train[i]=torch.from_numpy(X.transpose()[i:i+ar_order,:])\n",
    "        my_data_y_train[i]=torch.from_numpy(X.transpose()[i+ar_order,:])\n",
    "\n",
    "    for i in range(TS-ar_order-val_num, TS-ar_order,1):\n",
    "        my_data_val[i-(TS-ar_order-val_num)]=torch.from_numpy(X.transpose()[i:i+ar_order,:])\n",
    "        my_data_y_val[i-(TS-ar_order-val_num)]=torch.from_numpy(X.transpose()[i+ar_order,:])\n",
    "    return my_data_train, my_data_y_train, my_data_val, my_data_y_val\n",
    "\n",
    "X_np=[np.load(\"datasets/NETSIM/sim3_subject_%s.npz\" % (dataset_id))['X_np'] for dataset_id in range(50) ]\n",
    "Gref=[np.load(\"datasets/NETSIM/sim3_subject_%s.npz\" % (dataset_id))['Gref']*1.0 for dataset_id in range(50) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 3: split the data into training and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([198, 30])\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train , X_test, Y_test=multivariate_split(X=X_np[0],ar_order=2)\n",
    "\n",
    "X_train=torch.flatten(X_train, start_dim=1)\n",
    "X_test=torch.flatten(X_test, start_dim=1)\n",
    "\n",
    "Y_train=torch.flatten(Y_train, start_dim=1)\n",
    "Y_test=torch.flatten(Y_test, start_dim=1)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 4: Scale the data using StandardScaler class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the following link: https://www.hackerearth.com/blog/developers/radial-basis-function-network/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler= StandardScaler()\n",
    "# scaler.fit(X_train)\n",
    "# X_train= scaler.transform(X_train)\n",
    "# X_test= scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 5: Determine centers of the neurons using KMeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_cent= 8\n",
    "km= KMeans(n_clusters= K_cent, max_iter= 100, random_state=123)\n",
    "km.fit(X_train)\n",
    "cent= km.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 6: Determine the value of  $\\sigma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.006994436527214\n"
     ]
    }
   ],
   "source": [
    "max=0 \n",
    "for i in range(K_cent):\n",
    "    for j in range(K_cent):\n",
    "        d= numpy.linalg.norm(cent[i]-cent[j])\n",
    "        if(d> max):\n",
    "            max= d\n",
    "d= max\n",
    "\n",
    "sigma= d/math.sqrt(2*K_cent)\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 8: Set up matrix G."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape= X_train.shape\n",
    "row= shape[0]\n",
    "column= K_cent\n",
    "G= numpy.empty((row,column), dtype= float)\n",
    "for i in range(row):\n",
    "    for j in range(column):\n",
    "        dist= numpy.linalg.norm(X_train[i]-cent[j])\n",
    "        G[i][j]= math.exp(-math.pow(dist,2)/math.pow(2*sigma,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 9: Find weight matrix $W$ to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "GTG= numpy.dot(G.T,G)\n",
    "GTG_inv= numpy.linalg.inv(GTG)\n",
    "fac= numpy.dot(GTG_inv,G.T)\n",
    "W= numpy.dot(fac,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 10: Set up matrix G for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "row= X_test.shape[0]\n",
    "column= K_cent\n",
    "G_test= numpy.empty((row,column), dtype= float)\n",
    "for i in range(row):\n",
    "\tfor j in range(column):\n",
    "\t\tdist= numpy.linalg.norm(X_test[i]-cent[j])\n",
    "\t\tG_test[i][j]= math.exp(-math.pow(dist,2)/math.pow(2*sigma,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 11: Analyze the accuracy of prediction on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 15)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction= numpy.dot(G_test,W)\n",
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 15), dtype=float64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction-np.array(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am using in for Granger causality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "in_data_name='datasets/7SYNTHETICS/logistic_2.mat'\n",
    "\n",
    "GroundTruth=[loadmat(in_data_name)['Adj'] for i in range(50)]\n",
    "ts_logistic=[loadmat(in_data_name)['pt_N'][i,:,-500:] for i in range(50)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_normalized=normalize_0_mean_1_std(inp_series=ts_logistic[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Split the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([498, 6])\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train , X_test, Y_test=multivariate_split(X=X_normalized,ar_order=2)\n",
    "\n",
    "X_train=torch.flatten(X_train, start_dim=1)\n",
    "X_test=torch.flatten(X_test, start_dim=1)\n",
    "\n",
    "Y_train=torch.flatten(Y_train, start_dim=1)\n",
    "Y_test=torch.flatten(Y_test, start_dim=1)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Step 4: Obtain $k_f$ number of cluster centers with k-means clustering. It has dimensions of $ \\in k_f \\times Nd$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 6)\n"
     ]
    }
   ],
   "source": [
    "k_f=3\n",
    "km= KMeans(n_clusters= k_f, max_iter= 100, random_state=123)\n",
    "km.fit(X_train)\n",
    "cent= km.cluster_centers_\n",
    "\n",
    "print(cent.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Set the width of the kernel as the mean distance between cluster centers. (I select the max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3067340234321232\n"
     ]
    }
   ],
   "source": [
    "max=0 \n",
    "\n",
    "for i in range(k_f):\n",
    "    for j in range(k_f):\n",
    "        d= numpy.linalg.norm(cent[i]-cent[j])\n",
    "        if(d> max):\n",
    "            max= d\n",
    "d= max\n",
    "\n",
    "sigma= d/math.sqrt(2*k_f)\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Iterate through all $N$ time-series from 1 to $n$ where $n \\in N$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_d=np.zeros((np.shape(X_normalized)[0],np.shape(X_normalized)[0]));\n",
    "sig=np.zeros((np.shape(X_normalized)[0],np.shape(X_normalized)[0]));\n",
    "\n",
    "ar_order=2\n",
    "for i in range(X_normalized.shape[0]):\n",
    "    Z_temp=X_normalized.copy()\n",
    "    Z_train, Z_train_label , _ , _=multivariate_split(X=Z_temp,ar_order=2)\n",
    "    Z_train=torch.flatten(Z_train, start_dim=1)\n",
    "    Z_train_label=torch.flatten(Z_train_label, start_dim=1)\n",
    "    \n",
    "    # Obtain phase space Z_s by exclusing time series of of x_s\n",
    "    Z_s_train, Z_s_train_label , _ , _=multivariate_split(X=np.delete(Z_temp,[i],axis=0),ar_order=2)\n",
    "    # Obtain phase space reconstruction of x_s\n",
    "    W_s_train, W_s_train_label , _ , _=multivariate_split(X=np.array([Z_temp[i]]),ar_order=2)\n",
    "    \n",
    "    # Flatten data\n",
    "    Z_s_train=torch.flatten(Z_s_train, start_dim=1)\n",
    "    Z_s_train_label=torch.flatten(Z_s_train_label, start_dim=1)\n",
    "\n",
    "    W_s_train=torch.flatten(W_s_train, start_dim=1)\n",
    "    W_s_train_label=torch.flatten(W_s_train_label, start_dim=1)\n",
    "    # Obtain k_g number of cluster centers in the phase space W_s with k-means clustering, will have dim=(k_g * d)\n",
    "    k_g=2\n",
    "    kmg= KMeans(n_clusters= k_g, max_iter= 100, random_state=123)\n",
    "    kmg.fit(W_s_train)\n",
    "    cent_W_s= kmg.cluster_centers_\n",
    "    # Calculate activations for each of the k_g neurons\n",
    "    shape= W_s_train.shape\n",
    "    row= shape[0]\n",
    "    column= k_g\n",
    "    G= numpy.empty((row,column), dtype= float)\n",
    "    for ii in range(row):\n",
    "        for jj in range(column):\n",
    "            dist= numpy.linalg.norm(W_s_train[ii]-cent_W_s[jj])\n",
    "            G[ii][jj]= math.exp(-math.pow(dist,2)/math.pow(2*sigma,2))\n",
    "    # Generalized radial basis function\n",
    "    g_ws=np.array([G[ii]/sum(G[ii]) for ii in range(len(G))])\n",
    "    # Calculate activations for each of the k_f neurons \n",
    "    shape= Z_s_train.shape\n",
    "    row= shape[0]\n",
    "    column= k_f\n",
    "    F= numpy.empty((row,column), dtype= float)\n",
    "    for ii in range(row):\n",
    "        for jj in range(column):\n",
    "            cent_temp=cent.copy()\n",
    "            cent_temp=np.delete(cent_temp,np.arange(jj,jj+ar_order),axis=1)\n",
    "            dist= numpy.linalg.norm(Z_s_train[ii]-cent_temp)\n",
    "            F[ii][jj]= math.exp(-math.pow(dist,2)/math.pow(2*sigma,2))\n",
    "    # Generalized radial basis function\n",
    "    f_zs=np.array([F[ii]/sum(F[ii]) for ii in range(len(F))])\n",
    "    \n",
    "    # Prediction in the presence of x_s\n",
    "    dims=np.max([k_f,k_g])\n",
    "    num_samples=f_zs.shape[0]\n",
    "    f_new=np.ones((num_samples,dims))\n",
    "    g_new=np.ones((num_samples,dims))\n",
    "\n",
    "    for ii in range(f_new.shape[0]):\n",
    "        f_new[ii,:f_zs.shape[1]]=f_zs[ii,:]\n",
    "        g_new[ii,:g_ws.shape[1]]=g_ws[ii,:]\n",
    "    fg_combined=np.concatenate((g_new,f_new),axis=1)\n",
    "\n",
    "    GTG= numpy.dot(fg_combined.T,fg_combined)\n",
    "    GTG_inv= numpy.linalg.inv(GTG)\n",
    "    fac= numpy.dot(GTG_inv,fg_combined.T)\n",
    "    W_presence= numpy.dot(fac,Z_train_label)\n",
    "\n",
    "    prediction_presence= numpy.dot(fg_combined,W_presence)\n",
    "    error_presence=prediction_presence-np.array(Z_train_label)\n",
    "    sig[i,:]=np.diag(np.cov(error_presence.T))\n",
    "    \n",
    "    # Prediction without x_s\n",
    "    GTG= numpy.dot(f_zs.T,f_zs)\n",
    "    GTG_inv= numpy.linalg.inv(GTG)\n",
    "    fac= numpy.dot(GTG_inv,f_zs.T)\n",
    "    W_absence= numpy.dot(fac,Z_train_label)\n",
    "\n",
    "    prediction_absence= numpy.dot(f_zs,W_absence)\n",
    "    error_absence=prediction_absence-np.array(Z_train_label)\n",
    "    sig_d[i,:]=np.diag(np.cov(error_absence.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comupte the Granger causality index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 1.71823134e+00, 5.87995642e-01],\n",
       "       [2.82846679e-04, 0.00000000e+00, 6.71086205e-04],\n",
       "       [2.01776956e-04, 5.94881749e-03, 0.00000000e+00]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Aff=np.log((sig_d/sig))\n",
    "Aff=(Aff>0)*Aff\n",
    "np.fill_diagonal(Aff,0)\n",
    "Aff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GroundTruth[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6 (g): Predictions in the presence of $x_s$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims=np.max([k_f,k_g])\n",
    "num_samples=f_zs.shape[0]\n",
    "f_new=np.ones((num_samples,dims))\n",
    "g_new=np.ones((num_samples,dims))\n",
    "\n",
    "for ii in range(f_new.shape[0]):\n",
    "    f_new[ii,:f_zs.shape[1]]=f_zs[ii,:]\n",
    "    g_new[ii,:g_ws.shape[1]]=g_ws[ii,:]\n",
    "fg_combined=np.concatenate((g_new,f_new),axis=1)\n",
    "\n",
    "GTG= numpy.dot(fg_combined.T,fg_combined)\n",
    "GTG_inv= numpy.linalg.inv(GTG)\n",
    "fac= numpy.dot(GTG_inv,fg_combined.T)\n",
    "W_presence= numpy.dot(fac,Z_train_label)\n",
    "\n",
    "prediction_presence= numpy.dot(fg_combined,W_presence)\n",
    "error_presence=prediction_presence-np.array(Z_train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims=k_f+k_g\n",
    "num_samples=f_zs.shape[0]\n",
    "f_new=np.ones((num_samples,dims))\n",
    "\n",
    "for ii in range(f_new.shape[0]):\n",
    "    f_new[ii,:k_f]=f_zs[ii,:]\n",
    "    f_new[ii,k_f:]=g_ws[ii,:]\n",
    "\n",
    "GTG= numpy.dot(f_new.T,f_new)\n",
    "GTG_inv= numpy.linalg.inv(GTG)\n",
    "fac= numpy.dot(GTG_inv,f_new.T)\n",
    "W_presence= numpy.dot(fac,Z_train_label)\n",
    "\n",
    "prediction_presence= numpy.dot(f_new,W_presence)\n",
    "error_presence=prediction_presence-np.array(Z_train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6 (h): Predictions in the absence of $x_s$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "GTG= numpy.dot(f_zs.T,f_zs)\n",
    "GTG_inv= numpy.linalg.inv(GTG)\n",
    "fac= numpy.dot(GTG_inv,f_zs.T)\n",
    "W_absence= numpy.dot(fac,Z_train_label)\n",
    "\n",
    "prediction_absence= numpy.dot(f_zs,W_absence)\n",
    "error_absence=prediction_absence-np.array(Z_train_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
